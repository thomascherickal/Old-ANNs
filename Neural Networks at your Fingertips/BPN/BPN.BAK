

#include "neurons.h"



class BPN {

private:
	Neuron Input_Layer[NO_INPUTS];
	Neuron Hidden_Layer[NO_HIDDEN];
	Neuron Output_Layer[NO_OUTPUTS];

	DP Outputs[NO_OUTPUTS];
	DP Total_Error;
	DP Hidden_Outputs[NO_HIDDEN];
	DP Hidden_Error[NO_HIDDEN];
	DP Output_Error[NO_OUTPUTS];

	void ActivateNet(int pattern_no);
	void BackPropogate(int pattern_no);

public:
	BPN (void);
	DP Error(int pattern_no);
	void Display();
	void Train(void);
	
};

BPN::BPN(void)
{
	int i;
	for (i = 0; i < NO_INPUTS; i++) 
		Input_Layer[i] =  Neuron(Input_Weights[i], NO_INPUTS);
	for (i = 0; i < NO_HIDDEN; i++) 
		Hidden_Layer[i] =  Neuron(Hidden_Weights[i], NO_HIDDEN);
	for (i = 0; i < NO_OUTPUTS; i++) 
		Output_Layer[i] =  Neuron(Output_Weights[i], NO_HIDDEN);
}

void BPN::ActivateNet(int pattern_no)
{

	for (int i = 0; i < NO_INPUTS; i++) 
		Input_Layer[i].SetOutput(Inputs[pattern_no][i]);

	for (i = 0; i < NO_HIDDEN; i++) {
		Hidden_Layer[i].SetInputs(Inputs[pattern_no]);
		Hidden_Layer[i].Activate(Hidden_Layer[i].Summation() );
	}

	for (i = 0; i < NO_HIDDEN; i++) 
		Hidden_Outputs[i] = Hidden_Layer[i].GetOutput();

	for (i = 0; i < NO_OUTPUTS; i++) 
		Output_Layer[i].SetInputs(Hidden_Outputs);

	for (i = 0; i < NO_OUTPUTS; i++)
		Outputs[i] = Output_Layer[i].Activate(Output_Layer[i].Summation());		

}

void BPN::BackPropogate(int pattern_no)
{
	DP Sum[NO_HIDDEN] = { 0, 0, 0, 0 };
	DP d;
	
	for (int i = 0; i < NO_OUTPUTS; i++)
		Output_Error[i] = exp(-(d = fabs(Desired_Outputs[pattern_no][1] - Output_Layer[i].GetOutput()))) * ( 1 - exp(-d));
	
	for (i = 0; i < NO_OUTPUTS; i++)
		Output_Layer[i].UpdateWeights(Output_Error[0]);
	
	for (i = 0; i < NO_HIDDEN; i++)
		for (int j = 0; j < NO_HIDDEN; j++) 
			Sum[i] += Output_Error[0] * Hidden_Layer[i].GetWeights()[j];

	for ( i = 0; i < NO_HIDDEN; i++)
		Hidden_Error[i] = exp(-fabs(Sum[i])
		) * ( 1 - exp(-fabs(Sum[i]))) ;

	for ( i = 0; i < NO_HIDDEN; i++)
		Hidden_Layer[i].UpdateWeights(Hidden_Error[i]);


}

DP BPN::Error(int pattern_no)
{
	return 0.5 * pow(Outputs[0] - Desired_Outputs[pattern_no][1], 2);
}


void BPN::Train(void)
{
	DP Total_Error = 1;
	static int counter;
	while (abs(Total_Error) >= 0.001 && counter < 500000) {
		Total_Error = 0;
		for (int i = 0; i < NO_PATTERNS; i++) {
			ActivateNet(i);
			BackPropogate(i);
			Total_Error += Error(i);
		}
		counter++; 
	}
	cout << counter << endl;
	Display();
}

void BPN::Display(void)
{

	cout << endl << "HIDDEN WEIGHTS" << endl;

	for (int i = 0; i < NO_HIDDEN; i++) {
		for(int j = 0; j < NO_HIDDEN; j++)
			cout << Hidden_Layer[i].GetWeights()[j] << "  ";
		cout << endl;
	}
	cout << endl  << "OUTPUTS "<< endl;

	cout << Outputs[i];
	cout << endl;
	cin.get();
}

void main()
{
	BPN b;
	b.Train();
}